{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp engineering.nbdev\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove\n",
    "GloVe: Global Vectors for Word Representation\n",
    "\n",
    "GloVe是一种用于获取单词向量表示的无监督学习算法。 对来自语料库的汇总全局单词-单词共现统计信息进行训练，并且所得表示形式展示了单词向量空间的有趣线性子结构。\n",
    "\n",
    "\n",
    "GloVe的训练目标是学习单词向量，使其点积等于单词共现概率的对数。 由于比率的对数等于对数之差(log(a/b)=log(a)-log(b)，因此该目标将共现概率的比率（对数）与词向量空间中的向量差相关联。 因为这些比率可以编码某种形式的含义，所以此信息也被编码为矢量差。 因此，生成的词向量在词类比任务（例如在word2vec程序包中检查的类词）上的性能非常好。\n",
    "\n",
    "官网https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "https://github.com/stanfordnlp/GloVe\n",
    "\n",
    "## 原理\n",
    "https://blog.csdn.net/u014665013/article/details/79642083\n",
    "\n",
    "https://www.cnblogs.com/jfdwd/p/11086914.html\n",
    "\n",
    "* 首先基于语料库构建词的共现矩阵，\n",
    "* 然后基于共现矩阵和GloVe模型学习词向量。\n",
    "\n",
    "** 开始 -> 统计共现矩阵 -> 训练词向量 -> 结束**\n",
    "### 统计共现矩阵\n",
    "所谓的共现，共同出现，其实就是看一个词有没有在另一个词的附近出现，所谓的附近，其实就是一个移动窗口的概念，定义窗口的半径（从中心词到边缘的距离）后，看看方圆多少范围内出现词的个数，就是共现，现在看看例子。\n",
    "\n",
    "假设语料库就只有下面一行：\n",
    "\n",
    "    i love you but you love him i am sad\n",
    "\n",
    "设半径为2，于是移动窗口的滑动就有下面的形式：\n",
    "\n",
    "以窗口5为例，此处就可以认为，love分别和but, you, him, i共同出现了一次，通过这种方式去计数，就能知道任意两个词之间的共现关系（一般是可逆的），构成共现矩阵X，一般地，X是一个对称矩阵。\n",
    "\n",
    "### 使用GloVe模型训练词向量\n",
    "\n",
    "首先，模型的损失函数长这样的：\n",
    "$$J=\\sum_{i,j}^{N}f(X_{ij})(v_i^Tv_j+b_j+b_i-log(X_{ij}))^2$$\n",
    "\n",
    "vi和vj是词汇i和j的词向量，bi和bj是常数项，f是特定的权重函数，N是词汇表大小。\n",
    "\n",
    "X_ij的意义为：在整个语料库中，单词i和单词j共同出现在一个窗口中的次数。\n",
    "\n",
    "## 官网glove的使用\n",
    "\n",
    "The demo.sh script downloads a small corpus, consisting of the first 100M characters of Wikipedia. It collects unigram counts, constructs and shuffles cooccurrence data, and trains a simple version of the GloVe model. \n",
    "\n",
    "It also runs a word analogy evaluation script in python to verify word vector quality. More details about training on your own corpus can be found by reading demo.sh or the src/README.md\n",
    "\n",
    "初始化参数\n",
    "\n",
    "    CORPUS=text8\n",
    "    VOCAB_FILE=vocab.txt\n",
    "    COOCCURRENCE_FILE=cooccurrence.bin\n",
    "    COOCCURRENCE_SHUF_FILE=cooccurrence.shuf.bin\n",
    "    BUILDDIR=build\n",
    "    SAVE_FILE=vectors\n",
    "    VERBOSE=2\n",
    "    MEMORY=4.0\n",
    "    VOCAB_MIN_COUNT=5\n",
    "    VECTOR_SIZE=50\n",
    "    MAX_ITER=15\n",
    "    WINDOW_SIZE=15\n",
    "    \n",
    "运行会新生成五个文件\n",
    "\n",
    "    cooccurrence.bin \n",
    "    cooccurrence.shuf.bin\n",
    "    vectors.bin\n",
    "    vectors.txt  # 词向量\n",
    "    vocab.txt  # 词汇表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE               cooccurrence.shuf.bin \u001b[31mtext8\u001b[m\u001b[m\r\n",
      "Makefile              \u001b[31mdemo.sh\u001b[m\u001b[m               text8.zip\r\n",
      "README.md             \u001b[1m\u001b[36meval\u001b[m\u001b[m                  vectors.bin\r\n",
      "\u001b[1m\u001b[36mbuild\u001b[m\u001b[m                 \u001b[31mrandomization.test.sh\u001b[m\u001b[m vectors.txt\r\n",
      "cooccurrence.bin      \u001b[1m\u001b[36msrc\u001b[m\u001b[m                   vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/luoyonggui/Documents/temp/GloVe/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p build\n",
      "gcc -c src/vocab_count.c -o build/vocab_count.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "clang: \u001b[0;1;35mwarning: \u001b[0m-lm: 'linker' input unused [-Wunused-command-line-argument]\u001b[0m\n",
      "gcc -c src/cooccur.c -o build/cooccur.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "clang: \u001b[0;1;35mwarning: \u001b[0m-lm: 'linker' input unused [-Wunused-command-line-argument]\u001b[0m\n",
      "gcc -c src/shuffle.c -o build/shuffle.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "clang: \u001b[0;1;35mwarning: \u001b[0m-lm: 'linker' input unused [-Wunused-command-line-argument]\u001b[0m\n",
      "gcc -c src/glove.c -o build/glove.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "clang: \u001b[0;1;35mwarning: \u001b[0m-lm: 'linker' input unused [-Wunused-command-line-argument]\u001b[0m\n",
      "gcc -c src/common.c -o build/common.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "clang: \u001b[0;1;35mwarning: \u001b[0m-lm: 'linker' input unused [-Wunused-command-line-argument]\u001b[0m\n",
      "gcc build/vocab_count.o build/common.o -o build/vocab_count -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/cooccur.o build/common.o -o build/cooccur -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/shuffle.o build/common.o -o build/shuffle -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/glove.o build/common.o -o build/glove -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n"
     ]
    }
   ],
   "source": [
    "!cd /Users/luoyonggui/Documents/temp/GloVe/ && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p build\n",
      "\n",
      "$ build/vocab_count -min-count 5 -verbose 2 < text8 > vocab.txt\n",
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[11G300000 tokens.\u001b[11G400000 tokens.\u001b[11G500000 tokens.\u001b[11G600000 tokens.\u001b[11G700000 tokens.\u001b[11G800000 tokens.\u001b[11G900000 tokens.\u001b[11G1000000 tokens.\u001b[11G1100000 tokens.\u001b[11G1200000 tokens.\u001b[11G1300000 tokens.\u001b[11G1400000 tokens.\u001b[11G1500000 tokens.\u001b[11G1600000 tokens.\u001b[11G1700000 tokens.\u001b[11G1800000 tokens.\u001b[11G1900000 tokens.\u001b[11G2000000 tokens.\u001b[11G2100000 tokens.\u001b[11G2200000 tokens.\u001b[11G2300000 tokens.\u001b[11G2400000 tokens.\u001b[11G2500000 tokens.\u001b[11G2600000 tokens.\u001b[11G2700000 tokens.\u001b[11G2800000 tokens.\u001b[11G2900000 tokens.\u001b[11G3000000 tokens.\u001b[11G3100000 tokens.\u001b[11G3200000 tokens.\u001b[11G3300000 tokens.\u001b[11G3400000 tokens.\u001b[11G3500000 tokens.\u001b[11G3600000 tokens.\u001b[11G3700000 tokens.\u001b[11G3800000 tokens.\u001b[11G3900000 tokens.\u001b[11G4000000 tokens.\u001b[11G4100000 tokens.\u001b[11G4200000 tokens.\u001b[11G4300000 tokens.\u001b[11G4400000 tokens.\u001b[11G4500000 tokens.\u001b[11G4600000 tokens.\u001b[11G4700000 tokens.\u001b[11G4800000 tokens.\u001b[11G4900000 tokens.\u001b[11G5000000 tokens.\u001b[11G5100000 tokens.\u001b[11G5200000 tokens.\u001b[11G5300000 tokens.\u001b[11G5400000 tokens.\u001b[11G5500000 tokens.\u001b[11G5600000 tokens.\u001b[11G5700000 tokens.\u001b[11G5800000 tokens.\u001b[11G5900000 tokens.\u001b[11G6000000 tokens.\u001b[11G6100000 tokens.\u001b[11G6200000 tokens.\u001b[11G6300000 tokens.\u001b[11G6400000 tokens.\u001b[11G6500000 tokens.\u001b[11G6600000 tokens.\u001b[11G6700000 tokens.\u001b[11G6800000 tokens.\u001b[11G6900000 tokens.\u001b[11G7000000 tokens.\u001b[11G7100000 tokens.\u001b[11G7200000 tokens.\u001b[11G7300000 tokens.\u001b[11G7400000 tokens.\u001b[11G7500000 tokens.\u001b[11G7600000 tokens.\u001b[11G7700000 tokens.\u001b[11G7800000 tokens.\u001b[11G7900000 tokens.\u001b[11G8000000 tokens.\u001b[11G8100000 tokens.\u001b[11G8200000 tokens.\u001b[11G8300000 tokens.\u001b[11G8400000 tokens.\u001b[11G8500000 tokens.\u001b[11G8600000 tokens.\u001b[11G8700000 tokens.\u001b[11G8800000 tokens.\u001b[11G8900000 tokens.\u001b[11G9000000 tokens.\u001b[11G9100000 tokens.\u001b[11G9200000 tokens.\u001b[11G9300000 tokens.\u001b[11G9400000 tokens.\u001b[11G9500000 tokens.\u001b[11G9600000 tokens.\u001b[11G9700000 tokens.\u001b[11G9800000 tokens.\u001b[11G9900000 tokens.\u001b[11G10000000 tokens.\u001b[11G10100000 tokens.\u001b[11G10200000 tokens.\u001b[11G10300000 tokens.\u001b[11G10400000 tokens.\u001b[11G10500000 tokens.\u001b[11G10600000 tokens.\u001b[11G10700000 tokens.\u001b[11G10800000 tokens.\u001b[11G10900000 tokens.\u001b[11G11000000 tokens.\u001b[11G11100000 tokens.\u001b[11G11200000 tokens.\u001b[11G11300000 tokens.\u001b[11G11400000 tokens.\u001b[11G11500000 tokens.\u001b[11G11600000 tokens.\u001b[11G11700000 tokens.\u001b[11G11800000 tokens.\u001b[11G11900000 tokens.\u001b[11G12000000 tokens.\u001b[11G12100000 tokens.\u001b[11G12200000 tokens.\u001b[11G12300000 tokens.\u001b[11G12400000 tokens.\u001b[11G12500000 tokens.\u001b[11G12600000 tokens.\u001b[11G12700000 tokens.\u001b[11G12800000 tokens.\u001b[11G12900000 tokens.\u001b[11G13000000 tokens.\u001b[11G13100000 tokens.\u001b[11G13200000 tokens.\u001b[11G13300000 tokens.\u001b[11G13400000 tokens.\u001b[11G13500000 tokens.\u001b[11G13600000 tokens.\u001b[11G13700000 tokens.\u001b[11G13800000 tokens.\u001b[11G13900000 tokens.\u001b[11G14000000 tokens.\u001b[11G14100000 tokens.\u001b[11G14200000 tokens.\u001b[11G14300000 tokens.\u001b[11G14400000 tokens.\u001b[11G14500000 tokens.\u001b[11G14600000 tokens.\u001b[11G14700000 tokens.\u001b[11G14800000 tokens.\u001b[11G14900000 tokens.\u001b[11G15000000 tokens.\u001b[11G15100000 tokens.\u001b[11G15200000 tokens.\u001b[11G15300000 tokens.\u001b[11G15400000 tokens.\u001b[11G15500000 tokens.\u001b[11G15600000 tokens.\u001b[11G15700000 tokens.\u001b[11G15800000 tokens.\u001b[11G15900000 tokens.\u001b[11G16000000 tokens.\u001b[11G16100000 tokens.\u001b[11G16200000 tokens.\u001b[11G16300000 tokens.\u001b[11G16400000 tokens.\u001b[11G16500000 tokens.\u001b[11G16600000 tokens.\u001b[11G16700000 tokens.\u001b[11G16800000 tokens.\u001b[11G16900000 tokens.\u001b[11G17000000 tokens.\u001b[0GProcessed 17005207 tokens.\n",
      "Counted 253854 unique words.\n",
      "Truncating vocabulary at min count 5.\n",
      "Using vocabulary of size 71290.\n",
      "\n",
      "$ build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 < text8 > cooccurrence.bin\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"vocab.txt\"...loaded 71290 words.\n",
      "Building lookup table...table contains 94990279 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[19G1200000\u001b[19G1300000\u001b[19G1400000\u001b[19G1500000\u001b[19G1600000\u001b[19G1700000\u001b[19G1800000\u001b[19G1900000\u001b[19G2000000\u001b[19G2100000\u001b[19G2200000\u001b[19G2300000\u001b[19G2400000\u001b[19G2500000\u001b[19G2600000\u001b[19G2700000\u001b[19G2800000\u001b[19G2900000\u001b[19G3000000\u001b[19G3100000\u001b[19G3200000\u001b[19G3300000\u001b[19G3400000\u001b[19G3500000\u001b[19G3600000\u001b[19G3700000\u001b[19G3800000\u001b[19G3900000\u001b[19G4000000\u001b[19G4100000\u001b[19G4200000\u001b[19G4300000\u001b[19G4400000\u001b[19G4500000\u001b[19G4600000\u001b[19G4700000\u001b[19G4800000\u001b[19G4900000\u001b[19G5000000\u001b[19G5100000\u001b[19G5200000\u001b[19G5300000\u001b[19G5400000\u001b[19G5500000\u001b[19G5600000\u001b[19G5700000\u001b[19G5800000\u001b[19G5900000\u001b[19G6000000\u001b[19G6100000\u001b[19G6200000\u001b[19G6300000\u001b[19G6400000\u001b[19G6500000\u001b[19G6600000\u001b[19G6700000\u001b[19G6800000\u001b[19G6900000\u001b[19G7000000\u001b[19G7100000\u001b[19G7200000\u001b[19G7300000\u001b[19G7400000\u001b[19G7500000\u001b[19G7600000\u001b[19G7700000\u001b[19G7800000\u001b[19G7900000\u001b[19G8000000\u001b[19G8100000\u001b[19G8200000\u001b[19G8300000\u001b[19G8400000\u001b[19G8500000\u001b[19G8600000\u001b[19G8700000\u001b[19G8800000\u001b[19G8900000\u001b[19G9000000\u001b[19G9100000\u001b[19G9200000\u001b[19G9300000\u001b[19G9400000\u001b[19G9500000\u001b[19G9600000\u001b[19G9700000\u001b[19G9800000\u001b[19G9900000\u001b[19G10000000\u001b[19G10100000\u001b[19G10200000\u001b[19G10300000\u001b[19G10400000\u001b[19G10500000\u001b[19G10600000\u001b[19G10700000\u001b[19G10800000\u001b[19G10900000\u001b[19G11000000\u001b[19G11100000\u001b[19G11200000\u001b[19G11300000\u001b[19G11400000\u001b[19G11500000\u001b[19G11600000\u001b[19G11700000\u001b[19G11800000\u001b[19G11900000\u001b[19G12000000\u001b[19G12100000\u001b[19G12200000\u001b[19G12300000\u001b[19G12400000\u001b[19G12500000\u001b[19G12600000\u001b[19G12700000\u001b[19G12800000\u001b[19G12900000\u001b[19G13000000\u001b[19G13100000\u001b[19G13200000\u001b[19G13300000\u001b[19G13400000\u001b[19G13500000\u001b[19G13600000\u001b[19G13700000\u001b[19G13800000\u001b[19G13900000\u001b[19G14000000\u001b[19G14100000\u001b[19G14200000\u001b[19G14300000\u001b[19G14400000\u001b[19G14500000\u001b[19G14600000\u001b[19G14700000\u001b[19G14800000\u001b[19G14900000\u001b[19G15000000\u001b[19G15100000\u001b[19G15200000\u001b[19G15300000\u001b[19G15400000\u001b[19G15500000\u001b[19G15600000\u001b[19G15700000\u001b[19G15800000\u001b[19G15900000\u001b[19G16000000\u001b[19G16100000\u001b[19G16200000\u001b[19G16300000\u001b[19G16400000\u001b[19G16500000\u001b[19G16600000\u001b[19G16700000\u001b[19G16800000\u001b[19G16900000\u001b[19G17000000\u001b[0GProcessed 17005207 tokens.\n",
      "Writing cooccurrences to disk.........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[39G6300000 lines.\u001b[39G6400000 lines.\u001b[39G6500000 lines.\u001b[39G6600000 lines.\u001b[39G6700000 lines.\u001b[39G6800000 lines.\u001b[39G6900000 lines.\u001b[39G7000000 lines.\u001b[39G7100000 lines.\u001b[39G7200000 lines.\u001b[39G7300000 lines.\u001b[39G7400000 lines.\u001b[39G7500000 lines.\u001b[39G7600000 lines.\u001b[39G7700000 lines.\u001b[39G7800000 lines.\u001b[39G7900000 lines.\u001b[39G8000000 lines.\u001b[39G8100000 lines.\u001b[39G8200000 lines.\u001b[39G8300000 lines.\u001b[39G8400000 lines.\u001b[39G8500000 lines.\u001b[39G8600000 lines.\u001b[39G8700000 lines.\u001b[39G8800000 lines.\u001b[39G8900000 lines.\u001b[39G9000000 lines.\u001b[39G9100000 lines.\u001b[39G9200000 lines.\u001b[39G9300000 lines.\u001b[39G9400000 lines.\u001b[39G9500000 lines.\u001b[39G9600000 lines.\u001b[39G9700000 lines.\u001b[39G9800000 lines.\u001b[39G9900000 lines.\u001b[39G10000000 lines.\u001b[39G10100000 lines.\u001b[39G10200000 lines.\u001b[39G10300000 lines.\u001b[39G10400000 lines.\u001b[39G10500000 lines.\u001b[39G10600000 lines.\u001b[39G10700000 lines.\u001b[39G10800000 lines.\u001b[39G10900000 lines.\u001b[39G11000000 lines.\u001b[39G11100000 lines.\u001b[39G11200000 lines.\u001b[39G11300000 lines.\u001b[39G11400000 lines.\u001b[39G11500000 lines.\u001b[39G11600000 lines.\u001b[39G11700000 lines.\u001b[39G11800000 lines.\u001b[39G11900000 lines.\u001b[39G12000000 lines.\u001b[39G12100000 lines.\u001b[39G12200000 lines.\u001b[39G12300000 lines.\u001b[39G12400000 lines.\u001b[39G12500000 lines.\u001b[39G12600000 lines.\u001b[39G12700000 lines.\u001b[39G12800000 lines.\u001b[39G12900000 lines.\u001b[39G13000000 lines.\u001b[39G13100000 lines.\u001b[39G13200000 lines.\u001b[39G13300000 lines.\u001b[39G13400000 lines.\u001b[39G13500000 lines.\u001b[39G13600000 lines.\u001b[39G13700000 lines.\u001b[39G13800000 lines.\u001b[39G13900000 lines.\u001b[39G14000000 lines.\u001b[39G14100000 lines.\u001b[39G14200000 lines.\u001b[39G14300000 lines.\u001b[39G14400000 lines.\u001b[39G14500000 lines.\u001b[39G14600000 lines.\u001b[39G14700000 lines.\u001b[39G14800000 lines.\u001b[39G14900000 lines.\u001b[39G15000000 lines.\u001b[39G15100000 lines.\u001b[39G15200000 lines.\u001b[39G15300000 lines.\u001b[39G15400000 lines.\u001b[39G15500000 lines.\u001b[39G15600000 lines.\u001b[39G15700000 lines.\u001b[39G15800000 lines.\u001b[39G15900000 lines.\u001b[39G16000000 lines.\u001b[39G16100000 lines.\u001b[39G16200000 lines.\u001b[39G16300000 lines.\u001b[39G16400000 lines.\u001b[39G16500000 lines.\u001b[39G16600000 lines.\u001b[39G16700000 lines.\u001b[39G16800000 lines.\u001b[39G16900000 lines.\u001b[39G17000000 lines.\u001b[39G17100000 lines.\u001b[39G17200000 lines.\u001b[39G17300000 lines.\u001b[39G17400000 lines.\u001b[39G17500000 lines.\u001b[39G17600000 lines.\u001b[39G17700000 lines.\u001b[39G17800000 lines.\u001b[39G17900000 lines.\u001b[39G18000000 lines.\u001b[39G18100000 lines.\u001b[39G18200000 lines.\u001b[39G18300000 lines.\u001b[39G18400000 lines.\u001b[39G18500000 lines.\u001b[39G18600000 lines.\u001b[39G18700000 lines.\u001b[39G18800000 lines.\u001b[39G18900000 lines.\u001b[39G19000000 lines.\u001b[39G19100000 lines.\u001b[39G19200000 lines.\u001b[39G19300000 lines.\u001b[39G19400000 lines.\u001b[39G19500000 lines.\u001b[39G19600000 lines.\u001b[39G19700000 lines.\u001b[39G19800000 lines.\u001b[39G19900000 lines.\u001b[39G20000000 lines.\u001b[39G20100000 lines.\u001b[39G20200000 lines.\u001b[39G20300000 lines.\u001b[39G20400000 lines.\u001b[39G20500000 lines.\u001b[39G20600000 lines.\u001b[39G20700000 lines.\u001b[39G20800000 lines.\u001b[39G20900000 lines.\u001b[39G21000000 lines.\u001b[39G21100000 lines.\u001b[39G21200000 lines.\u001b[39G21300000 lines.\u001b[39G21400000 lines.\u001b[39G21500000 lines.\u001b[39G21600000 lines.\u001b[39G21700000 lines.\u001b[39G21800000 lines.\u001b[39G21900000 lines.\u001b[39G22000000 lines.\u001b[39G22100000 lines.\u001b[39G22200000 lines.\u001b[39G22300000 lines.\u001b[39G22400000 lines.\u001b[39G22500000 lines.\u001b[39G22600000 lines.\u001b[39G22700000 lines.\u001b[39G22800000 lines.\u001b[39G22900000 lines.\u001b[39G23000000 lines.\u001b[39G23100000 lines.\u001b[39G23200000 lines.\u001b[39G23300000 lines.\u001b[39G23400000 lines.\u001b[39G23500000 lines.\u001b[39G23600000 lines.\u001b[39G23700000 lines.\u001b[39G23800000 lines.\u001b[39G23900000 lines.\u001b[39G24000000 lines.\u001b[39G24100000 lines.\u001b[39G24200000 lines.\u001b[39G24300000 lines.\u001b[39G24400000 lines.\u001b[39G24500000 lines.\u001b[39G24600000 lines.\u001b[39G24700000 lines.\u001b[39G24800000 lines.\u001b[39G24900000 lines.\u001b[39G25000000 lines.\u001b[39G25100000 lines.\u001b[39G25200000 lines.\u001b[39G25300000 lines.\u001b[39G25400000 lines.\u001b[39G25500000 lines.\u001b[39G25600000 lines.\u001b[39G25700000 lines.\u001b[39G25800000 lines.\u001b[39G25900000 lines.\u001b[39G26000000 lines.\u001b[39G26100000 lines.\u001b[39G26200000 lines.\u001b[39G26300000 lines.\u001b[39G26400000 lines.\u001b[39G26500000 lines.\u001b[39G26600000 lines.\u001b[39G26700000 lines.\u001b[39G26800000 lines.\u001b[39G26900000 lines.\u001b[39G27000000 lines.\u001b[39G27100000 lines.\u001b[39G27200000 lines.\u001b[39G27300000 lines.\u001b[39G27400000 lines.\u001b[39G27500000 lines.\u001b[39G27600000 lines.\u001b[39G27700000 lines.\u001b[39G27800000 lines.\u001b[39G27900000 lines.\u001b[39G28000000 lines.\u001b[39G28100000 lines.\u001b[39G28200000 lines.\u001b[39G28300000 lines.\u001b[39G28400000 lines.\u001b[39G28500000 lines.\u001b[39G28600000 lines.\u001b[39G28700000 lines.\u001b[39G28800000 lines.\u001b[39G28900000 lines.\u001b[39G29000000 lines.\u001b[39G29100000 lines.\u001b[39G29200000 lines.\u001b[39G29300000 lines.\u001b[39G29400000 lines.\u001b[39G29500000 lines.\u001b[39G29600000 lines.\u001b[39G29700000 lines.\u001b[39G29800000 lines.\u001b[39G29900000 lines.\u001b[39G30000000 lines.\u001b[39G30100000 lines.\u001b[39G30200000 lines.\u001b[39G30300000 lines.\u001b[39G30400000 lines.\u001b[39G30500000 lines.\u001b[39G30600000 lines.\u001b[39G30700000 lines.\u001b[39G30800000 lines.\u001b[39G30900000 lines.\u001b[39G31000000 lines.\u001b[39G31100000 lines.\u001b[39G31200000 lines.\u001b[39G31300000 lines.\u001b[39G31400000 lines.\u001b[39G31500000 lines.\u001b[39G31600000 lines.\u001b[39G31700000 lines.\u001b[39G31800000 lines.\u001b[39G31900000 lines.\u001b[39G32000000 lines.\u001b[39G32100000 lines.\u001b[39G32200000 lines.\u001b[39G32300000 lines.\u001b[39G32400000 lines.\u001b[39G32500000 lines.\u001b[39G32600000 lines.\u001b[39G32700000 lines.\u001b[39G32800000 lines.\u001b[39G32900000 lines.\u001b[39G33000000 lines.\u001b[39G33100000 lines.\u001b[39G33200000 lines.\u001b[39G33300000 lines.\u001b[39G33400000 lines.\u001b[39G33500000 lines.\u001b[39G33600000 lines.\u001b[39G33700000 lines.\u001b[39G33800000 lines.\u001b[39G33900000 lines.\u001b[39G34000000 lines.\u001b[39G34100000 lines.\u001b[39G34200000 lines.\u001b[39G34300000 lines.\u001b[39G34400000 lines.\u001b[39G34500000 lines.\u001b[39G34600000 lines.\u001b[39G34700000 lines.\u001b[39G34800000 lines.\u001b[39G34900000 lines.\u001b[39G35000000 lines.\u001b[39G35100000 lines.\u001b[39G35200000 lines.\u001b[39G35300000 lines.\u001b[39G35400000 lines.\u001b[39G35500000 lines.\u001b[39G35600000 lines.\u001b[39G35700000 lines.\u001b[39G35800000 lines.\u001b[39G35900000 lines.\u001b[39G36000000 lines.\u001b[39G36100000 lines.\u001b[39G36200000 lines.\u001b[39G36300000 lines.\u001b[39G36400000 lines.\u001b[39G36500000 lines.\u001b[39G36600000 lines.\u001b[39G36700000 lines.\u001b[39G36800000 lines.\u001b[39G36900000 lines.\u001b[39G37000000 lines.\u001b[39G37100000 lines.\u001b[39G37200000 lines.\u001b[39G37300000 lines.\u001b[39G37400000 lines.\u001b[39G37500000 lines.\u001b[39G37600000 lines.\u001b[39G37700000 lines.\u001b[39G37800000 lines.\u001b[39G37900000 lines.\u001b[39G38000000 lines.\u001b[39G38100000 lines.\u001b[39G38200000 lines.\u001b[39G38300000 lines.\u001b[39G38400000 lines.\u001b[39G38500000 lines.\u001b[39G38600000 lines.\u001b[39G38700000 lines.\u001b[39G38800000 lines.\u001b[39G38900000 lines.\u001b[39G39000000 lines.\u001b[39G39100000 lines.\u001b[39G39200000 lines.\u001b[39G39300000 lines.\u001b[39G39400000 lines.\u001b[39G39500000 lines.\u001b[39G39600000 lines.\u001b[39G39700000 lines.\u001b[39G39800000 lines.\u001b[39G39900000 lines.\u001b[39G40000000 lines.\u001b[39G40100000 lines.\u001b[39G40200000 lines.\u001b[39G40300000 lines.\u001b[39G40400000 lines.\u001b[39G40500000 lines.\u001b[39G40600000 lines.\u001b[39G40700000 lines.\u001b[39G40800000 lines.\u001b[39G40900000 lines.\u001b[39G41000000 lines.\u001b[39G41100000 lines.\u001b[39G41200000 lines.\u001b[39G41300000 lines.\u001b[39G41400000 lines.\u001b[39G41500000 lines.\u001b[39G41600000 lines.\u001b[39G41700000 lines.\u001b[39G41800000 lines.\u001b[39G41900000 lines.\u001b[39G42000000 lines.\u001b[39G42100000 lines.\u001b[39G42200000 lines.\u001b[39G42300000 lines.\u001b[39G42400000 lines.\u001b[39G42500000 lines.\u001b[39G42600000 lines.\u001b[39G42700000 lines.\u001b[39G42800000 lines.\u001b[39G42900000 lines.\u001b[39G43000000 lines.\u001b[39G43100000 lines.\u001b[39G43200000 lines.\u001b[39G43300000 lines.\u001b[39G43400000 lines.\u001b[39G43500000 lines.\u001b[39G43600000 lines.\u001b[39G43700000 lines.\u001b[39G43800000 lines.\u001b[39G43900000 lines.\u001b[39G44000000 lines.\u001b[39G44100000 lines.\u001b[39G44200000 lines.\u001b[39G44300000 lines.\u001b[39G44400000 lines.\u001b[39G44500000 lines.\u001b[39G44600000 lines.\u001b[39G44700000 lines.\u001b[39G44800000 lines.\u001b[39G44900000 lines.\u001b[39G45000000 lines.\u001b[39G45100000 lines.\u001b[39G45200000 lines.\u001b[39G45300000 lines.\u001b[39G45400000 lines.\u001b[39G45500000 lines.\u001b[39G45600000 lines.\u001b[39G45700000 lines.\u001b[39G45800000 lines.\u001b[39G45900000 lines.\u001b[39G46000000 lines.\u001b[39G46100000 lines.\u001b[39G46200000 lines.\u001b[39G46300000 lines.\u001b[39G46400000 lines.\u001b[39G46500000 lines.\u001b[39G46600000 lines.\u001b[39G46700000 lines.\u001b[39G46800000 lines.\u001b[39G46900000 lines.\u001b[39G47000000 lines.\u001b[39G47100000 lines.\u001b[39G47200000 lines.\u001b[39G47300000 lines.\u001b[39G47400000 lines.\u001b[39G47500000 lines.\u001b[39G47600000 lines.\u001b[39G47700000 lines.\u001b[39G47800000 lines.\u001b[39G47900000 lines.\u001b[39G48000000 lines.\u001b[39G48100000 lines.\u001b[39G48200000 lines.\u001b[39G48300000 lines.\u001b[39G48400000 lines.\u001b[39G48500000 lines.\u001b[39G48600000 lines.\u001b[39G48700000 lines.\u001b[39G48800000 lines.\u001b[39G48900000 lines.\u001b[39G49000000 lines.\u001b[39G49100000 lines.\u001b[39G49200000 lines.\u001b[39G49300000 lines.\u001b[39G49400000 lines.\u001b[39G49500000 lines.\u001b[39G49600000 lines.\u001b[39G49700000 lines.\u001b[39G49800000 lines.\u001b[39G49900000 lines.\u001b[39G50000000 lines.\u001b[39G50100000 lines.\u001b[39G50200000 lines.\u001b[39G50300000 lines.\u001b[39G50400000 lines.\u001b[39G50500000 lines.\u001b[39G50600000 lines.\u001b[39G50700000 lines.\u001b[39G50800000 lines.\u001b[39G50900000 lines.\u001b[39G51000000 lines.\u001b[39G51100000 lines.\u001b[39G51200000 lines.\u001b[39G51300000 lines.\u001b[39G51400000 lines.\u001b[39G51500000 lines.\u001b[39G51600000 lines.\u001b[39G51700000 lines.\u001b[39G51800000 lines.\u001b[39G51900000 lines.\u001b[39G52000000 lines.\u001b[39G52100000 lines.\u001b[39G52200000 lines.\u001b[39G52300000 lines.\u001b[39G52400000 lines.\u001b[39G52500000 lines.\u001b[39G52600000 lines.\u001b[39G52700000 lines.\u001b[39G52800000 lines.\u001b[39G52900000 lines.\u001b[39G53000000 lines.\u001b[39G53100000 lines.\u001b[39G53200000 lines.\u001b[39G53300000 lines.\u001b[39G53400000 lines.\u001b[39G53500000 lines.\u001b[39G53600000 lines.\u001b[39G53700000 lines.\u001b[39G53800000 lines.\u001b[39G53900000 lines.\u001b[39G54000000 lines.\u001b[39G54100000 lines.\u001b[39G54200000 lines.\u001b[39G54300000 lines.\u001b[39G54400000 lines.\u001b[39G54500000 lines.\u001b[39G54600000 lines.\u001b[39G54700000 lines.\u001b[39G54800000 lines.\u001b[39G54900000 lines.\u001b[39G55000000 lines.\u001b[39G55100000 lines.\u001b[39G55200000 lines.\u001b[39G55300000 lines.\u001b[39G55400000 lines.\u001b[39G55500000 lines.\u001b[39G55600000 lines.\u001b[39G55700000 lines.\u001b[39G55800000 lines.\u001b[39G55900000 lines.\u001b[39G56000000 lines.\u001b[39G56100000 lines.\u001b[39G56200000 lines.\u001b[39G56300000 lines.\u001b[39G56400000 lines.\u001b[39G56500000 lines.\u001b[39G56600000 lines.\u001b[39G56700000 lines.\u001b[39G56800000 lines.\u001b[39G56900000 lines.\u001b[39G57000000 lines.\u001b[39G57100000 lines.\u001b[39G57200000 lines.\u001b[39G57300000 lines.\u001b[39G57400000 lines.\u001b[39G57500000 lines.\u001b[39G57600000 lines.\u001b[39G57700000 lines.\u001b[39G57800000 lines.\u001b[39G57900000 lines.\u001b[39G58000000 lines.\u001b[39G58100000 lines.\u001b[39G58200000 lines.\u001b[39G58300000 lines.\u001b[39G58400000 lines.\u001b[39G58500000 lines.\u001b[39G58600000 lines.\u001b[39G58700000 lines.\u001b[39G58800000 lines.\u001b[39G58900000 lines.\u001b[39G59000000 lines.\u001b[39G59100000 lines.\u001b[39G59200000 lines.\u001b[39G59300000 lines.\u001b[39G59400000 lines.\u001b[39G59500000 lines.\u001b[39G59600000 lines.\u001b[39G59700000 lines.\u001b[39G59800000 lines.\u001b[39G59900000 lines.\u001b[39G60000000 lines.\u001b[39G60100000 lines.\u001b[39G60200000 lines.\u001b[39G60300000 lines.\u001b[39G60400000 lines.\u001b[39G60500000 lines.\u001b[39G60600000 lines.\u001b[0GMerging cooccurrence files: processed 60666468 lines.\n",
      "\n",
      "$ build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin\n",
      "Using random seed 1589640604\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 60666468 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G60666468 lines.\u001b[0GMerging temp files: processed 60666468 lines.\n",
      "\n",
      "$ build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max 10 -iter 15 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2\n",
      "TRAINING MODEL\n",
      "Read 60666468 lines.\n",
      "Initializing parameters...Using random seed 1589640635\n",
      "done.\n",
      "vector size: 50\n",
      "vocab size: 71290\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "05/16/20 - 10:50.51PM, iter: 001, cost: 0.071219\n",
      "05/16/20 - 10:51.08PM, iter: 002, cost: 0.052709\n",
      "05/16/20 - 10:51.24PM, iter: 003, cost: 0.046688\n",
      "05/16/20 - 10:51.41PM, iter: 004, cost: 0.043389\n",
      "05/16/20 - 10:51.58PM, iter: 005, cost: 0.041459\n",
      "05/16/20 - 10:52.16PM, iter: 006, cost: 0.040194\n",
      "05/16/20 - 10:52.33PM, iter: 007, cost: 0.039290\n",
      "05/16/20 - 10:52.49PM, iter: 008, cost: 0.038602\n",
      "05/16/20 - 10:53.04PM, iter: 009, cost: 0.038062\n",
      "05/16/20 - 10:53.20PM, iter: 010, cost: 0.037622\n",
      "05/16/20 - 10:53.36PM, iter: 011, cost: 0.037259\n",
      "05/16/20 - 10:53.53PM, iter: 012, cost: 0.036953\n",
      "05/16/20 - 10:54.12PM, iter: 013, cost: 0.036688\n",
      "05/16/20 - 10:54.28PM, iter: 014, cost: 0.036459\n",
      "05/16/20 - 10:54.47PM, iter: 015, cost: 0.036258\n",
      "$ python eval/python/evaluate.py\n",
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 58.50% (296/506)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 25.95% (925/3564)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 4.53% (27/596)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 24.33% (567/2330)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 37.62% (158/420)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 4.23% (42/992)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 2.12% (16/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 24.70% (329/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 8.57% (85/992)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 11.74% (124/1056)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 55.88% (850/1521)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 11.92% (186/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 25.38% (338/1332)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 8.97% (78/870)\n",
      "Questions seen/total: 91.21% (17827/19544)\n",
      "Semantic accuracy: 26.60%  (1973/7416)\n",
      "Syntactic accuracy: 19.67%  (2048/10411)\n",
      "Total accuracy: 22.56%  (4021/17827)\n"
     ]
    }
   ],
   "source": [
    "!cd /Users/luoyonggui/Documents/temp/GloVe/ && ./demo.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe的Python实现\n",
    "\n",
    "在pypi里面看到了很多GloVe的包，但是很多都有坑，我直接说一个我自己已经走通的包mittens。\n",
    "\n",
    "下载方式还是比较简单的， pip install mittens基本没什么问题，想要去看看源码的话，在这里：\n",
    "\n",
    "一般而言GloVe按照计算共现矩阵和GloVe训练两大模块，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - glove_python\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove_python\n",
      "  Using cached https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from glove_python) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from glove_python) (1.4.1)\n",
      "Building wheels for collected packages: glove-python\n",
      "  Building wheel for glove-python (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Complete output from command /Users/luoyonggui/anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='\"'\"'/private/var/folders/7j/kgtjln3x2dj2g2v57d5vncyw0000gp/T/pip-install-krt5v7vc/glove-python/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/7j/kgtjln3x2dj2g2v57d5vncyw0000gp/T/pip-wheel-7a5f_xz3 --python-tag cp37:\u001b[0m\n",
      "\u001b[31m  ERROR: running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.9-x86_64-3.7\n",
      "  creating build/lib.macosx-10.9-x86_64-3.7/glove\n",
      "  copying glove/__init__.py -> build/lib.macosx-10.9-x86_64-3.7/glove\n",
      "  copying glove/glove.py -> build/lib.macosx-10.9-x86_64-3.7/glove\n",
      "  copying glove/corpus.py -> build/lib.macosx-10.9-x86_64-3.7/glove\n",
      "  running build_ext\n",
      "  building 'glove.glove_cython' extension\n",
      "  creating build/temp.macosx-10.9-x86_64-3.7\n",
      "  creating build/temp.macosx-10.9-x86_64-3.7/glove\n",
      "  x86_64-apple-darwin13.4.0-clang -DNDEBUG -fwrapv -O3 -Wall -Wstrict-prototypes -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -D_FORTIFY_SOURCE=2 -mmacosx-version-min=10.9 -I/Users/luoyonggui/anaconda3/include/python3.7m -c glove/glove_cython.c -o build/temp.macosx-10.9-x86_64-3.7/glove/glove_cython.o -fopenmp -ffast-math -march=native\n",
      "  glove/glove_cython.c:262:10: fatal error: 'omp.h' file not found\n",
      "  #include <omp.h>\n",
      "           ^~~~~~~\n",
      "  1 error generated.\n",
      "  error: command 'x86_64-apple-darwin13.4.0-clang' failed with exit status 1\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for glove-python\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for glove-python\n",
      "\u001b[31m  ERROR: Complete output from command /Users/luoyonggui/anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='\"'\"'/private/var/folders/7j/kgtjln3x2dj2g2v57d5vncyw0000gp/T/pip-install-krt5v7vc/glove-python/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' clean --all:\u001b[0m\n",
      "\u001b[31m  ERROR: usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
      "     or: -c --help [cmd1 cmd2 ...]\n",
      "     or: -c --help-commands\n",
      "     or: -c cmd --help\n",
      "  \n",
      "  error: option --all not recognized\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed cleaning build dir for glove-python\u001b[0m\n",
      "Failed to build glove-python\n",
      "Installing collected packages: glove-python\n",
      "  Running setup.py install for glove-python ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Complete output from command /Users/luoyonggui/anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='\"'\"'/private/var/folders/7j/kgtjln3x2dj2g2v57d5vncyw0000gp/T/pip-install-krt5v7vc/glove-python/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/7j/kgtjln3x2dj2g2v57d5vncyw0000gp/T/pip-record-7o_g4kw6/install-record.txt --single-version-externally-managed --compile:\u001b[0m\n",
      "\u001b[31m    ERROR: running install\n",
      "    running build\n",
      "    running build_py\n",
      "    running build_ext\n",
      "    building 'glove.glove_cython' extension\n",
      "    x86_64-apple-darwin13.4.0-clang -DNDEBUG -fwrapv -O3 -Wall -Wstrict-prototypes -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -D_FORTIFY_SOURCE=2 -mmacosx-version-min=10.9 -I/Users/luoyonggui/anaconda3/include/python3.7m -c glove/glove_cython.c -o build/temp.macosx-10.9-x86_64-3.7/glove/glove_cython.o -fopenmp -ffast-math -march=native\n",
      "    glove/glove_cython.c:262:10: fatal error: 'omp.h' file not found\n",
      "    #include <omp.h>\n",
      "             ^~~~~~~\n",
      "    1 error generated.\n",
      "    error: command 'x86_64-apple-darwin13.4.0-clang' failed with exit status 1\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command \"/Users/luoyonggui/anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='\"'\"'/private/var/folders/7j/kgtjln3x2dj2g2v57d5vncyw0000gp/T/pip-install-krt5v7vc/glove-python/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/7j/kgtjln3x2dj2g2v57d5vncyw0000gp/T/pip-record-7o_g4kw6/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/7j/kgtjln3x2dj2g2v57d5vncyw0000gp/T/pip-install-krt5v7vc/glove-python/\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install glove_python -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim加载glove训练的词向量\n",
    "https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "\n",
    "glove词向量的格式如下：\n",
    "\n",
    "    word1 0.123 0.134 0.532 0.152\n",
    "    word2 0.934 0.412 0.532 0.159\n",
    "    word3 0.334 0.241 0.324 0.18\n",
    "    ...\n",
    "    word9 0.334 0.241 0.324 0.188\n",
    "\n",
    "word2vec词向量的格式：\n",
    "\n",
    "    9 4   # 这一行包含向量的数量及其维度\n",
    "    word1 0.123 0.134 0.532 0.152\n",
    "    word2 0.934 0.412 0.532 0.159\n",
    "    word3 0.334 0.241 0.324 0.188\n",
    "    ...\n",
    "    word9 0.334 0.241 0.324 0.188\n",
    "\n",
    "gensim库添加了一个模块，可以用来将glove格式的词向量转为word2vec的词向量，具体操作如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "# 输入文件\n",
    "glove_file = datapath('/Users/luoyonggui/Documents/temp/GloVe/vectors.txt')\n",
    "# 输出文件\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "# call glove2word2vec script\n",
    "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "\n",
    "# 开始转换\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "# 加载转化后的文件\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.346661, -0.657651, -0.323899,  1.404478, -0.681525,  0.332714,\n",
       "        -0.493434,  0.950834,  0.102952,  0.386339,  0.347187, -0.912068,\n",
       "        -0.137993, -1.246887,  0.062835, -0.035913, -0.365426, -0.219618,\n",
       "        -0.971765,  1.701459,  0.939117,  0.349692, -0.533264,  1.316008,\n",
       "        -0.708478,  0.394686,  0.751405,  0.498659,  1.257347, -2.340782,\n",
       "        -0.162739,  0.501467,  1.076938, -0.577817,  0.409253,  0.1018  ,\n",
       "         0.634708, -0.831036,  0.519494,  0.491227, -0.013857,  0.245493,\n",
       "        -0.521764, -1.030283, -0.486644, -0.087301,  1.220634,  1.356461,\n",
       "         1.723411,  1.3197  ], dtype=float32), (50,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['the'], model['the'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "\n",
    "## bag-of-words model\n",
    "This model transforms each document to a fixed-length vector of integers. For example, given the sentences:\n",
    "\n",
    "    John likes to watch movies. Mary likes movies too.\n",
    "\n",
    "    John also likes to watch football games. Mary hates football.\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "    [1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "    [1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "首先，他们会丢失所有有关单词顺序的信息：“约翰喜欢玛丽”和“玛丽喜欢约翰”对应于相同的向量。 There is a solution: bag of n-grams models consider word phrases of length n to represent documents as fixed-length vectors to capture local word order but suffer from data sparsity and high dimensionality.\n",
    "\n",
    "其次，该模型不会尝试学习基础单词的含义，因此，向量之间的距离并不总是反映出含义上的差异。 \n",
    "\n",
    "Word2Vec模型解决了第二个问题。\n",
    "\n",
    "## the Word2Vec Model\n",
    "Word2Vec是一种更新的模型，它使用浅层神经网络将单词嵌入到低维向量空间中。 结果是一组词向量，其中在向量空间中靠在一起的向量根据上下文具有相似的含义，而彼此远离的词向量具有不同的含义。\n",
    "\n",
    "The are two versions of this model and Word2Vec class implements them both:\n",
    "\n",
    "![](img/sk01.png)\n",
    "### Skip-grams (SG)\n",
    "The Word2Vec Skip-gram model, for example, takes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the synthetic task of given an input word, giving us a predicted probability distribution of nearby words to the input. A virtual one-hot encoding of words goes through a ‘projection layer’ to the hidden layer; these projection weights are later interpreted as the word embeddings. So if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings.\n",
    "\n",
    "### Continuous-bag-of-words (CBOW)\n",
    "Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It is also a 1-hidden-layer neural network. The synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word. Again, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==3.7.2\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U gensim\n",
    "!pip freeze | grep gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 默认参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.Word2Vec(\n",
    "    sentences=None,\n",
    "    corpus_file=None,\n",
    "    size=100,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    max_vocab_size=None,\n",
    "    sample=0.001,\n",
    "    seed=1,\n",
    "    workers=3,\n",
    "    min_alpha=0.0001,\n",
    "    sg=0,\n",
    "    hs=0,\n",
    "    negative=5,\n",
    "    ns_exponent=0.75,\n",
    "    cbow_mean=1,\n",
    "    hashfxn=<built-in function hash>,\n",
    "    iter=5,\n",
    "    null_word=0,\n",
    "    trim_rule=None,\n",
    "    sorted_vocab=1,\n",
    "    batch_words=10000,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    max_final_vocab=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_count\n",
    "min_count用于修剪内部字典。 在十亿个单词的语料库中仅出现一次或两次的单词可能是无趣的错别字和垃圾。 此外，没有足够的数据来对这些词进行任何有意义的训练，因此最好忽略它们：\n",
    "\n",
    "min_count = 5的默认值\n",
    "#### size\n",
    "size是gensim Word2Vec将单词映射到的N维空间的维数（N）。\n",
    "\n",
    "较大的值需要更多的训练数据，但可以产生更好（更准确）的模型。 合理的值在数十到数百之间。\n",
    "\n",
    "default value of size=100\n",
    "#### worker\n",
    "worker，最后一个主要参数（此处为完整列表）用于训练并行化，以加快训练速度：\n",
    "\n",
    "default value of workers=3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Your Own Model\n",
    "这个语料库足够小，可以完全放入内存，但是我们将实现一个内存友好的迭代器，逐行读取它，以演示如何处理更大的语料库。\n",
    "\n",
    "如果我们想进行任何自定义的预处理，例如 解码非标准编码，小写字母，删除数字，提取命名实体……所有这些都可以在MyCorpus迭代器中完成，而word2vec不需要知道。 所需要的就是输入产生一个句子（另一个utf8单词列表）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hundreds',\n",
       " 'of',\n",
       " 'people',\n",
       " 'have',\n",
       " 'been',\n",
       " 'forced',\n",
       " 'to',\n",
       " 'vacate',\n",
       " 'their',\n",
       " 'homes']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.simple_preprocess('Hundreds of people have been forced to vacate their homes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeError: you must first build vocabulary before training the model\n",
    "# 原因:训练文本太小\n",
    "# sentences = [\n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"dog\", \"say\", \"woof\"]]\n",
    "# model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01486553,  0.01823401, -0.02772865,  0.00213301,  0.0372683 ,\n",
       "        0.01642814,  0.04816883, -0.05448446,  0.01253152,  0.03404462,\n",
       "        0.01756212, -0.04840738, -0.03839137, -0.01304129, -0.03804687,\n",
       "        0.05305994, -0.00860955, -0.00836853,  0.00134603, -0.03344008,\n",
       "       -0.03039226, -0.03281286, -0.00623441, -0.02607584,  0.00408613,\n",
       "        0.04711006,  0.03532708, -0.0055987 , -0.00130932,  0.02229508,\n",
       "       -0.04697058, -0.02922403,  0.0131788 , -0.01622746,  0.05389683,\n",
       "       -0.02905632,  0.03431457,  0.01364054,  0.00634684, -0.01930137,\n",
       "       -0.01475715, -0.04367147, -0.00581169, -0.03634231,  0.01892559,\n",
       "       -0.01157352,  0.02849645,  0.00408916,  0.03510711, -0.0240911 ,\n",
       "       -0.03448397, -0.00420031, -0.06505472,  0.00562562, -0.03190516,\n",
       "       -0.01666111, -0.04189085,  0.07402997,  0.00904749, -0.03249473,\n",
       "        0.00972437,  0.03047183, -0.00820827, -0.02223486, -0.01516857,\n",
       "       -0.00403475, -0.02458509,  0.02097399, -0.00187658,  0.01206905,\n",
       "       -0.04879121, -0.01009979, -0.024553  , -0.02041692, -0.01424486,\n",
       "        0.03416541,  0.01465811, -0.02790837, -0.05651117,  0.03401321,\n",
       "        0.05107525, -0.01604857,  0.04182722, -0.01838509,  0.00617696,\n",
       "       -0.017543  ,  0.02919475,  0.06655695,  0.04709207,  0.06052168,\n",
       "        0.02376736, -0.01636989,  0.01506402,  0.00921484, -0.00102513,\n",
       "       -0.01343615, -0.0189462 , -0.00384656,  0.01540534, -0.04651833],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king = model.wv['king']\n",
    "vec_king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01486553,  0.01823401, -0.02772865,  0.00213301,  0.0372683 ,\n",
       "        0.01642814,  0.04816883, -0.05448446,  0.01253152,  0.03404462,\n",
       "        0.01756212, -0.04840738, -0.03839137, -0.01304129, -0.03804687,\n",
       "        0.05305994, -0.00860955, -0.00836853,  0.00134603, -0.03344008,\n",
       "       -0.03039226, -0.03281286, -0.00623441, -0.02607584,  0.00408613,\n",
       "        0.04711006,  0.03532708, -0.0055987 , -0.00130932,  0.02229508,\n",
       "       -0.04697058, -0.02922403,  0.0131788 , -0.01622746,  0.05389683,\n",
       "       -0.02905632,  0.03431457,  0.01364054,  0.00634684, -0.01930137,\n",
       "       -0.01475715, -0.04367147, -0.00581169, -0.03634231,  0.01892559,\n",
       "       -0.01157352,  0.02849645,  0.00408916,  0.03510711, -0.0240911 ,\n",
       "       -0.03448397, -0.00420031, -0.06505472,  0.00562562, -0.03190516,\n",
       "       -0.01666111, -0.04189085,  0.07402997,  0.00904749, -0.03249473,\n",
       "        0.00972437,  0.03047183, -0.00820827, -0.02223486, -0.01516857,\n",
       "       -0.00403475, -0.02458509,  0.02097399, -0.00187658,  0.01206905,\n",
       "       -0.04879121, -0.01009979, -0.024553  , -0.02041692, -0.01424486,\n",
       "        0.03416541,  0.01465811, -0.02790837, -0.05651117,  0.03401321,\n",
       "        0.05107525, -0.01604857,  0.04182722, -0.01838509,  0.00617696,\n",
       "       -0.017543  ,  0.02919475,  0.06655695,  0.04709207,  0.06052168,\n",
       "        0.02376736, -0.01636989,  0.01506402,  0.00921484, -0.00102513,\n",
       "       -0.01343615, -0.0189462 , -0.00384656,  0.01540534, -0.04651833],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['king']  # removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'cameroon' does not appear in this model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vec_cameroon = model.wv['cameroon']\n",
    "except KeyError:\n",
    "    print(\"The word 'cameroon' does not appear in this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取最相近的词向量topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('away', 0.9970558881759644), ('sea', 0.9966898560523987)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('king', topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('have', 0.9996969699859619), ('five', 0.9996781349182129), ('israeli', 0.9996719360351562), ('just', 0.9996703863143921), ('other', 0.999667763710022)]\n"
     ]
    }
   ],
   "source": [
    "# Print the 5 most similar words to “car” or “minivan”\n",
    "print(model.wv.most_similar(positive=['car', 'man'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py:858: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'land'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hundreds\n",
      "of\n",
      "people\n",
      "have\n",
      "been\n",
      "forced\n",
      "to\n",
      "their\n",
      "homes\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/luoyonggui/PycharmProjects/nbdevlib'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model.save('/Users/luoyonggui/Downloads/tt.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "new_model = Word2Vec.load('/Users/luoyonggui/Downloads/tt.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01486553,  0.01823401, -0.02772865,  0.00213301,  0.0372683 ,\n",
       "        0.01642814,  0.04816883, -0.05448446,  0.01253152,  0.03404462,\n",
       "        0.01756212, -0.04840738, -0.03839137, -0.01304129, -0.03804687,\n",
       "        0.05305994, -0.00860955, -0.00836853,  0.00134603, -0.03344008,\n",
       "       -0.03039226, -0.03281286, -0.00623441, -0.02607584,  0.00408613,\n",
       "        0.04711006,  0.03532708, -0.0055987 , -0.00130932,  0.02229508,\n",
       "       -0.04697058, -0.02922403,  0.0131788 , -0.01622746,  0.05389683,\n",
       "       -0.02905632,  0.03431457,  0.01364054,  0.00634684, -0.01930137,\n",
       "       -0.01475715, -0.04367147, -0.00581169, -0.03634231,  0.01892559,\n",
       "       -0.01157352,  0.02849645,  0.00408916,  0.03510711, -0.0240911 ,\n",
       "       -0.03448397, -0.00420031, -0.06505472,  0.00562562, -0.03190516,\n",
       "       -0.01666111, -0.04189085,  0.07402997,  0.00904749, -0.03249473,\n",
       "        0.00972437,  0.03047183, -0.00820827, -0.02223486, -0.01516857,\n",
       "       -0.00403475, -0.02458509,  0.02097399, -0.00187658,  0.01206905,\n",
       "       -0.04879121, -0.01009979, -0.024553  , -0.02041692, -0.01424486,\n",
       "        0.03416541,  0.01465811, -0.02790837, -0.05651117,  0.03401321,\n",
       "        0.05107525, -0.01604857,  0.04182722, -0.01838509,  0.00617696,\n",
       "       -0.017543  ,  0.02919475,  0.06655695,  0.04709207,  0.06052168,\n",
       "        0.02376736, -0.01636989,  0.01506402,  0.00921484, -0.00102513,\n",
       "       -0.01343615, -0.0189462 , -0.00384656,  0.01540534, -0.04651833],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.wv['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nb_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted engineering_nbdev.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No notebooks were modified\r\n",
      "converting /Users/luoyonggui/PycharmProjects/nbdevlib/index.ipynb to README.md\r\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
